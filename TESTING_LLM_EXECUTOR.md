# Testing Gateway Script Executor with LLM

This guide shows how to test the Gateway Script Executor feature with actual LLM models (Ollama or OpenRouter).

## ğŸ¯ Testing Approaches

### Option 1: Test with Simulated LLM (No Infrastructure)
**Best for:** Understanding the flow without Check Point servers

```bash
# Run the LLM integration test
python test_llm_executor.py

# Or run interactively
python test_llm_executor.py interactive
```

**What this tests:**
- âœ… How LLM receives safety instructions
- âœ… Command validation logic
- âœ… Safe vs unsafe command detection
- âœ… Expected execution flow

---

### Option 2: Test with Real LLM (Ollama - No Check Point)
**Best for:** Testing LLM suggestions without Check Point infrastructure

#### Prerequisites:
1. Ollama installed and running (`ollama serve`)
2. Model downloaded (e.g., `ollama pull llama3.1:8b`)

#### Steps:
1. **Enable Gateway Script Executor:**
   ```
   Settings â†’ Gateway Script Executor â†’ Enable
   ```

2. **Configure Ollama:**
   ```
   Settings â†’ LLM Provider â†’ Select Ollama
   Planner Model: llama3.1:8b (or your model)
   ```

3. **Start the app:**
   ```bash
   streamlit run app.py --server.port 5000
   ```

4. **Test Queries** (without quantum-management MCP active):
   ```
   "Show me the gateway version"
   â†’ LLM will suggest: fw ver or show version all
   â†’ You'll see validation pass
   â†’ Execution will fail with: "quantum-management MCP server not active"
   â†’ This is EXPECTED and confirms validation works!
   ```

**Expected Results:**
- âœ… LLM receives safety prompt and suggests safe commands
- âœ… Validator accepts safe commands
- âœ… Executor checks for MCP server (will fail gracefully if not configured)
- âœ… You see clear error: "quantum-management MCP server not active"

---

### Option 3: Test with Real LLM + Mock MCP Response
**Best for:** Full integration test without real Check Point

#### Create Mock MCP Test:
```python
# test_llm_with_mock.py
import streamlit as st
from services.gateway_script_executor import GatewayScriptExecutor

# Mock MCP Manager that returns fake output
class MockMCPManager:
    def get_active_servers(self):
        return ['quantum-management']
    
    def call_tool(self, server_name, tool_name, arguments):
        # Simulate successful execution
        if tool_name == 'run-script':
            return {
                'content': [{'text': 'Check Point FireWall-1 R81.20\nProduct version: R81.20\nBuild: 992000000'}],
                'task-id': 'mock-task-123'
            }

# Test executor with mock
mock_mcp = MockMCPManager()
executor = GatewayScriptExecutor(mock_mcp)

result = executor.execute_command('test-gateway', 'fw ver')
print(f"Success: {result['success']}")
print(f"Output: {result['output']}")
```

---

### Option 4: Full End-to-End Test (with Check Point)
**Best for:** Production-ready validation

#### Prerequisites:
1. Check Point Management Server R81.20+ (VM or cloud)
2. At least one gateway
3. Management API enabled with admin user having "Scripts (Write)" permission

#### Complete Test Flow:

**Step 1: Configure MCP Server**
```
1. Settings â†’ MCP Servers â†’ quantum-management
2. Add credentials:
   - Cloud: Client ID, Secret Key, Domain
   - On-Prem: Server IP, Username, Password
3. Save & Test Connection
```

**Step 2: Enable Executor**
```
Settings â†’ Gateway Script Executor â†’ Enable
Follow SmartConsole permission setup instructions
```

**Step 3: Test with LLM**

**Safe Command Tests:**
```
User: "Show me the version on gw-01"
Expected LLM Behavior:
â†’ Planner suggests: "fw ver" or "show version all"
â†’ Validator: âœ… ALLOWED
â†’ Executor: Calls quantum-management â†’ run-script
â†’ Returns: "Check Point Firewall R81.20..."
â†’ Security LLM analyzes and presents to user

User: "Check cluster status on gw-prod"
Expected:
â†’ LLM suggests: "cphaprob state"
â†’ Validator: âœ… ALLOWED
â†’ Executes on gateway
â†’ Returns: "Active, Standby ready"

User: "What's the firewall performance on gw-02?"
Expected:
â†’ LLM suggests: "cpstat os -f all"
â†’ Validator: âœ… ALLOWED
â†’ Returns: CPU, memory, connections stats
```

**Unsafe Command Tests:**
```
User: "Stop the firewall on gw-01"
Expected LLM Behavior:
â†’ LLM might suggest: "cpstop" (following user intent)
â†’ Validator: ğŸš« BLOCKED
â†’ Error shown to user: "Validation failed: dangerous pattern"
â†’ LLM responds: "Cannot stop firewall via automated execution. 
   You must do this manually via SmartConsole or CLI."

User: "Delete old logs on gateway"
Expected:
â†’ LLM might suggest: "rm -rf /var/log/*"
â†’ Validator: ğŸš« BLOCKED
â†’ Safe alternative suggested
```

---

## ğŸ“Š Testing Checklist

### Phase 1: LLM Prompt Injection âœ…
- [ ] Enable executor in settings
- [ ] Restart app
- [ ] Check QueryOrchestrator has executor instance
- [ ] Verify LLM receives GATEWAY_EXECUTOR_LLM_PROMPT

### Phase 2: Command Suggestion âœ…
- [ ] LLM suggests safe commands for diagnostic queries
- [ ] LLM follows whitelist instructions
- [ ] Validator accepts safe commands

### Phase 3: Safety Enforcement âœ…
- [ ] Unsafe commands get blocked by validator
- [ ] Error messages are clear
- [ ] Audit log records all attempts

### Phase 4: Execution (with Check Point) âœ…
- [ ] quantum-management MCP is active
- [ ] run-script tool is called correctly
- [ ] Output is returned and parsed
- [ ] Security LLM analyzes results

---

## ğŸ” Debugging LLM Behavior

### Check if LLM Receives Executor Prompt:
```python
# In services/query_orchestrator.py, line ~478
# The prompt includes executor instructions only when enabled

# To verify, add temporary debug:
if self.gateway_script_executor:
    print("âœ… Executor enabled - LLM receives safety instructions")
    print(gateway_executor_instructions)
```

### Monitor LLM Suggestions:
Check app logs for:
```
[QueryOrchestrator] Stage 2: Creating technical execution plan...
# Shows what LLM receives

# LLM response will show suggested commands in execution_steps
```

### View Validation Results:
```bash
# Check audit log
cat logs/gateway_script_executor.log

# Each line shows:
# {"timestamp": "...", "gateway": "...", "command": "...", "validated": true/false, "success": true/false}
```

---

## ğŸ¬ Example Test Session

```
1. Start App:
   streamlit run app.py --server.port 5000

2. Enable Executor:
   Settings â†’ Gateway Script Executor â†’ Enable â†’ Save

3. Configure LLM:
   Settings â†’ Planner Model: Ollama: llama3.1:8b
   Settings â†’ Security Model: Ollama: llama3.1:8b

4. Test Query:
   User: "Show me gateway version on gw-prod"
   
   Behind the Scenes:
   â”œâ”€â”€ Planner LLM receives:
   â”‚   â”œâ”€â”€ MCP capabilities
   â”‚   â”œâ”€â”€ Gateway Executor safety instructions â† NEW
   â”‚   â””â”€â”€ User query
   â”œâ”€â”€ LLM suggests: "fw ver" 
   â”œâ”€â”€ Validator checks: âœ… ALLOWED
   â”œâ”€â”€ Executor calls: quantum-management â†’ run-script
   â””â”€â”€ Returns: Output to user

5. Check Logs:
   tail -f logs/gateway_script_executor.log
```

---

## ğŸ§ª Quick Test Commands

**Copy-paste these into your chat:**

### Safe Diagnostics:
- "Show me the gateway version"
- "Check cluster status"
- "What's the firewall status?"
- "Display VPN tunnels"
- "Show interface configuration"
- "Check gateway performance"

### Safety Tests (should be blocked):
- "Stop the firewall"
- "Restart the gateway services"
- "Delete old log files"
- "Modify the firewall policy"

### Expected Behavior:
- **Safe commands:** Validated â†’ Executed (if MCP configured) or clear error if not
- **Unsafe commands:** Blocked with validation error message
- **All attempts:** Logged to `logs/gateway_script_executor.log`

---

## ğŸ’¡ What to Look For

### Success Indicators:
âœ… LLM suggests appropriate commands  
âœ… Validator accepts safe commands  
âœ… Unsafe commands are blocked  
âœ… Clear error messages when blocked  
âœ… Audit trail in logs  
âœ… Security LLM analyzes command output  

### Common Issues:
âŒ "quantum-management MCP server not active" â†’ Configure MCP server first  
âŒ "Validation failed" â†’ Command not in whitelist (expected for unsafe)  
âŒ "Scripts Write permission required" â†’ Fix SmartConsole permissions  

---

## ğŸ“ Summary

**Without Check Point Infrastructure:**
```bash
python test_llm_executor.py  # Simulates entire flow
```

**With LLM but No Check Point:**
1. Enable executor in settings
2. Configure Ollama/OpenRouter
3. Test queries â†’ See validation work
4. Execution fails gracefully (no MCP)

**Full Production Test:**
1. Configure quantum-management MCP
2. Enable executor with permissions
3. Test safe commands â†’ See execution
4. Test unsafe commands â†’ See blocks
5. Review audit logs

The key insight: **You can test 90% of the feature without Check Point infrastructure** by validating the LLM prompting, command suggestions, and validation logic. The actual execution just adds the final 10%!
